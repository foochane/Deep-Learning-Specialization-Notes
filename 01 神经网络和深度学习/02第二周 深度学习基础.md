# 第一周 深度学习基础

## 2.0 内概概要
本周学习的内容有：
- 2.1 二分类（Binary Classification）
- 2.2 逻辑回归(Logistic Regression)
- 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）
- 2.4 梯度下降法（Gradient Descent）
- 2.5 导数（Derivatives）
- 2.6 更多的导数例子（More Derivative Examples）
- 2.7 计算图（Computation Graph）
- 2.8 计 算 图 的 导 数 计 算 （Derivatives with a Computation Graph）
- 2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）

## 2.1 二分类（Binary Classification）

一个二分类问题的例子，假如你有一张图片作为输入，如果识别这张图
片为猫，则输出标签 1 作为结果；如果识别出不是猫，那么输出标签 0 作为结果。现在我们
可以用字母y来表示输出的结果标签，如下图所示：

![](/image/0201.png)

为了保存一张图片，需要保存三个矩阵，
它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为 64x64 像素，那么你
就有三个规模为 64x64 的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。

![](/image/0202.png)

把这些像素值放到一个特征向量x中，如果图片的大小为64x64像素，那么例子中x的维度就是
64x64x3=12288，也就是n=12288.

在这个二分类问题中，我们就学习得到了一个分类器，它
以图片的特征向量作为输入，然后预测输出结果y,是 1 还是 0，也就是预测图片中是否有猫：

![](/image/0203.png)

## 2.2 逻辑回归(Logistic Regression)

逻辑回归算法适用于二分类问题，本节主要介绍逻辑回归的
 Hypothesis Function（假设函数）。


对于二元分类问题来讲，给定一个输入特征向量X，它可能对应一张图片，你想识别这
张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只
能称之为 ![](/image/y_hat.png)，也就是你对实际值y的估计。

更正式的说，就是当输入为X时， ![](/image/y_hat.gif) 表示y等于1的可能性，也就是
例子中输入图像有猫的概率。我们用w和b来表示逻辑回归的特征权值和偏差，当输入为x时，
将产生一个预测值 ![](/image/y_hat=w^Tx+b.png)

实际上，得到一个0和1之间的概率是没有多大意义的，因此在逻辑回归中，![](/image/y_hat.gif)只是sigmoid函数的自变量。

![](/image/0204.png)

## 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）

**为什么需要代价函数:**

为了训练逻辑回归模型的参数参w和b，需要一个代价函数，通过训练代价函数来得到参数参w和b。先看一下逻辑回归的输出函数：

![](/image/0205.png)

为了通过学习调整参数，我们需要一个大小为m的训练集，通过训练找到参数w和b。

对于训练集的预测值，我们写成 

![](/image/y_hat=sigma(z).png)

其中：

![](/image/z^(i)=sigma(w^Tx^(i)+b).png)


**损失函数:**

损失函数又叫做误差函数，用来衡量算法的运行情况， Loss function:
![](/image/L(y_hat,y).png)

我们称函数L为损失函数，用于衡量预测值和实际值之间的误差。

通常情况损失函数我们会使用均方误差，但在逻辑回归中我们经常使用另一个函数：

![](/image/lossfun1.png)


为什么要用这个函数作为逻辑损失函数？在使用均分误差的时候，目标函数不是凸优化的，只能找到局部最优值，使用梯度下降法很可能找不到全局最优值。

损失函数是在单个训练样本中定义的，它衡
量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们
需要定义一个算法的代价函数，算法的代价函数是对m个样本的损失函数求和然后除以m:

![](/image/0206.png)

## 2.4 梯度下降法（Gradient Descent）

在测试数据集上，通过最小化代价函数J(w,b)来训练参数w和b。

![](/image/0207.png)

迭代公式：

![](/image/0208.png)

## 2.5 导数（Derivatives）

## 2.6 更多的导数例子（More Derivative Examples）

## 2.7 计算图（Computation Graph）

一个神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出
一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对
应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。

![](/image/0209.png)

## 2.8 计 算 图 的 导 数 计 算 （Derivatives with a Computation Graph）

在上一节中，我们看了一个例子使用流程计算图来计算函数 J。现在我们清理一下
流程图的描述，看看你如何利用它计算出函数J的导数。

![](/image/0210.png)

![](/image/0211.png)

![](/image/0212.png)

![](/image/0213.png)

## 2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）

本节我们讨论怎样通过计算偏导数来实现逻辑回归的梯度下降算法。它的关键点是几个
重要公式，其作用是用来实现逻辑回归中梯度下降算法。但是在本节视频中，我将使用计算
图对梯度下降算法进行计算。我必须要承认的是，使用计算图来计算逻辑回归的梯度下降算
法有点大材小用了。但是，我认为以这个例子作为开始来讲解，可以使你更好的理解背后的
思想。从而在讨论神经网络时，你可以更深刻而全面地理解神经网络。接下来让我们开始学
习逻辑回归的梯度下降算法。

![](/image/0214.png)