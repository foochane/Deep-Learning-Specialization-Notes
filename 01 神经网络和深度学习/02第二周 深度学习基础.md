# 第一周 深度学习基础

## 2.0 内概概要
本周学习的内容有：
- 2.1 二分类（Binary Classification）
- 2.2 逻辑回归(Logistic Regression)
- 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）
- 2.4 梯度下降法（Gradient Descent）
- 2.5 导数（Derivatives）
- 2.6 更多的导数例子（More Derivative Examples）
- 2.7 计算图（Computation Graph）
- 2.8 计算图的导数计算 （Derivatives with a Computation Graph）
- 2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）
- 2.10 m 个样本的梯度下降(Gradient Descent on m Examples)
- 2.11 向量化(Vectorization)
- 2.12 向量化的更多例子（More Examples of Vectorization）
- 2.13 向量化逻辑回归(Vectorizing Logistic Regression)

## 2.1 二分类（Binary Classification）

一个二分类问题的例子，假如你有一张图片作为输入，如果识别这张图
片为猫，则输出标签 1 作为结果；如果识别出不是猫，那么输出标签 0 作为结果。现在我们
可以用字母y来表示输出的结果标签，如下图所示：

![](/image/0201.png)

为了保存一张图片，需要保存三个矩阵，
它们分别对应图片中的红、绿、蓝三种颜色通道，如果你的图片大小为 64x64 像素，那么你
就有三个规模为 64x64 的矩阵，分别对应图片中红、绿、蓝三种像素的强度值。

![](/image/0202.png)

把这些像素值放到一个特征向量x中，如果图片的大小为64x64像素，那么例子中x的维度就是
64x64x3=12288，也就是n=12288.

在这个二分类问题中，我们就学习得到了一个分类器，它
以图片的特征向量作为输入，然后预测输出结果y,是 1 还是 0，也就是预测图片中是否有猫：

![](/image/0203.png)

## 2.2 逻辑回归(Logistic Regression)

逻辑回归算法适用于二分类问题，本节主要介绍逻辑回归的
 Hypothesis Function（假设函数）。


对于二元分类问题来讲，给定一个输入特征向量X，它可能对应一张图片，你想识别这
张图片识别看它是否是一只猫或者不是一只猫的图片，你想要一个算法能够输出预测，你只
能称之为 ![y_hat](/image/0219.png)，也就是你对实际值y的估计。

更正式的说，就是当输入为X时， ![y_hat](/image/0219.png) 表示y等于1的可能性，也就是
例子中输入图像有猫的概率。我们用w和b来表示逻辑回归的特征权值和偏差，当输入为x时，
将产生一个预测值 ![y_hat=w^Tx+b](/image/0220.png)

实际上，得到一个0和1之间的概率是没有多大意义的，因此在逻辑回归中，![y_hat](/image/0219.png)只是sigmoid函数的自变量。

![](/image/0204.png)

## 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）

**为什么需要代价函数:**

为了训练逻辑回归模型的参数参w和b，需要一个代价函数，通过训练代价函数来得到参数参w和b。先看一下逻辑回归的输出函数：

![](/image/0205.png)

为了通过学习调整参数，我们需要一个大小为m的训练集，通过训练找到参数w和b。

对于训练集的预测值，我们写成 

![y_hat=sigma(z)](/image/0223.png)

其中：

![z^(i)=sigma(w^Tx^(i)+b](/image/0224.png)


**损失函数:**

损失函数又叫做误差函数，用来衡量算法的运行情况， Loss function:
![](/image/0221.png)

我们称函数L为损失函数，用于衡量预测值和实际值之间的误差。

通常情况损失函数我们会使用均方误差，但在逻辑回归中我们经常使用另一个函数：

![](/image/0222.png)


为什么要用这个函数作为逻辑损失函数？在使用均分误差的时候，目标函数不是凸优化的，只能找到局部最优值，使用梯度下降法很可能找不到全局最优值。

损失函数是在单个训练样本中定义的，它衡
量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们
需要定义一个算法的代价函数，算法的代价函数是对m个样本的损失函数求和然后除以m:

![](/image/0206.png)

## 2.4 梯度下降法（Gradient Descent）

在测试数据集上，通过最小化代价函数J(w,b)来训练参数w和b。

![](/image/0207.png)

迭代公式：

![](/image/0208.png)

## 2.5 导数（Derivatives）

## 2.6 更多的导数例子（More Derivative Examples）

## 2.7 计算图（Computation Graph）

一个神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出
一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对
应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。

![](/image/0209.png)

## 2.8 计 算 图 的 导 数 计 算 （Derivatives with a Computation Graph）

在上一节中，我们看了一个例子使用流程计算图来计算函数 J。现在我们清理一下
流程图的描述，看看你如何利用它计算出函数J的导数。

![](/image/0210.png)

![](/image/0211.png)

![](/image/0212.png)

![](/image/0213.png)

## 2.9 逻辑回归中的梯度下降（Logistic Regression Gradient Descent）

本节我们讨论怎样通过计算偏导数来实现逻辑回归的梯度下降算法。它的关键点是几个
重要公式，其作用是用来实现逻辑回归中梯度下降算法。但是在本节视频中，我将使用计算
图对梯度下降算法进行计算。我必须要承认的是，使用计算图来计算逻辑回归的梯度下降算
法有点大材小用了。但是，我认为以这个例子作为开始来讲解，可以使你更好的理解背后的
思想。从而在讨论神经网络时，你可以更深刻而全面地理解神经网络。接下来让我们开始学
习逻辑回归的梯度下降算法。

![](/image/0214.png)

![](/image/0215.png)

![](/image/0217.png)

![](/image/0216.png)

## 2.10 m 个样本的梯度下降(Gradient Descent on m Examples)

![](/image/0218.png)

记住关于损失函数J(w,b)的定义：

![](/image/0225.png)

![](/image/0226.png)

代码流程：

```
J=0
dw1=0
dw2=0
db=0

for i = 1 to m
    z(i) = wx(i)+b
    a(i) = sigmoid(z(i))
    J += -[y(i)log(a(i))+(1-y(i)） log(1-a(i))
    dz(i) = a(i)-y(i)
    dw1 += x1(i)dz(i)
    dw2 += x2(i)dz(i)
    db += dz(i)
J/= m
dw1/= m
dw2/= m
db/= m

w1=w1-alpha*dw1
w2=w2-alpha*dw2
b=b-alpha*db


```

计算中有两个缺点，也就是说应用此方法在逻辑回归上你需要编写两个 for 循环。
第一个 for 循环是一个小循环遍历m个训练样本，第二个 for 循环是遍历一个样本的所有特征值。这个例子中我们只有 2 个特征，所以n等于2。 但如果你有更多特征，
就需要计算dw1，dw2，dw3…… ，一直到dwn，直到遍历完n个特征值。

运用深度学习算法时，在代码中显示地使用for循环会使算法很低效。在训练很大的数据
集时，去掉显示的for循环是非常重要的。

这里有一种叫做向量化的技术，可以是代码摆脱显示的for循环。

## 2.11 向量化(Vectorization)

向量化是非常基础的去除代码中 for 循环的艺术，在深度学习安全领域、深度学习实践
中，你会经常发现自己训练大数据集，因为深度学习算法处理大数据集效果很棒，所以你的
代码运行速度非常重要，否则如果在大数据集上，你的代码可能花费很长时间去运行，你将
要等待非常长的时间去得到结果。所以在深度学习领域，运行向量化是一个关键的技巧。

一个向量化的例子：

代码

```
import numpy as np
import time

a = np.random.rand(10000000)
b = np.random.rand(10000000)

tic = time.time()
c = np.dot(a,b)
toc = time.time()
print(c)
print("Vectorized version:" + str(1000*(toc-tic)) + "ms")

c = 0
tic = time.time()
for i in range(10000000):
    c += a[i]*b[i]
toc = time.time()
print(c)
print("For loop:" + str(1000*(toc-tic)) + "ms")
```

运行结果为：
```
2500013.6685443483
Vectorized version:11.002063751220703ms
2500013.6685442952
For loop:6194.152116775513ms
```

## 2.12 向量化的更多例子（More Examples of Vectorization）

经验告诉我们，当我们在写神经网络程序时，或者在写逻辑(logistic)回归，或者其他神经
网络模型时，应该避免写循环(loop)语句。虽然有时写循环(loop)是不可避免的，但是我们可
以使用比如 numpy 的内置函数或者其他办法去计算。当你这样使用后，程序效率总是快于循环(loop)。

![](/image/0227.png)

## 2.13 向量化逻辑回归(Vectorizing Logistic Regression)

![](/image/0228.png)

![](/image/0229.png)