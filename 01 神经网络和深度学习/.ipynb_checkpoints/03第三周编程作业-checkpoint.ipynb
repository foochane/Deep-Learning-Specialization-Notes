{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三周编程作业\n",
    "\n",
    "在这周的作业里，你将会产生红色和蓝色的点来形成一朵花的图。然后你将训练一个神经网络来正确地分类这些点。您将尝试不同的层并查看结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 软件包\n",
    "\n",
    "- numpy：是用Python进行科学计算的基本软件包。\n",
    "- sklearn：为数据挖掘和数据分析提供的简单高效的工具。\n",
    "- matplotlib ：是一个用于在Python中绘制图表的库。\n",
    "- testCases：提供了一些测试示例来评估函数的正确性，参见下载的资料或者在底部查看它的代码。\n",
    "- planar_utils ：提供了在这个任务中使用的各种有用的功能，参见下载的资料或者在底部查看它的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
    "%matplotlib inline \n",
    "\n",
    "np.random.seed(1) #设置一个固定的随机种子，以保证接下来的步骤中我们的结果是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 数据集\n",
    "\n",
    "使用如下命令来加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把数据集加载完成了，然后使用matplotlib可视化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #绘制散点图\n",
    "plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  X：一个numpy的矩阵，包含了这些数据点的数值\n",
    "-  Y：一个numpy的向量，对应着的是X的标签【0 | 1】（红色:0 ， 蓝色 :1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = Y.shape[1]  # 训练集里面的数量\n",
    "\n",
    "print (\"X的维度为: \" + str(shape_X))\n",
    "print (\"Y的维度为: \" + str(shape_Y))\n",
    "print (\"数据集里面的数据有：\" + str(m) + \" 个\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 神经网络搭建\n",
    "\n",
    "我们的模型如下：\n",
    "\n",
    "![](../image/0321.png)\n",
    "\n",
    "公式如下：\n",
    "\n",
    "![](../image/0322.png)\n",
    "\n",
    "给出所以输入的预测结果，可以按如下方式来计算成本J:\n",
    "\n",
    "![](../image/0323.png)\n",
    "\n",
    "![](../image/0324.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04定义神经网络结构\n",
    "在构建之前，我们要先把神经网络的结构给定义好：\n",
    "\n",
    "- n_x: 输入层的数量\n",
    "- n_h: 隐藏层的数量（这里设置为4）\n",
    "- n_y: 输出层的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X , Y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "     X - 输入数据集,维度为（输入的数量，训练/测试的数量）\n",
    "     Y - 标签，维度为（输出的数量，训练/测试数量）\n",
    "\n",
    "    返回：\n",
    "     n_x - 输入层的数量\n",
    "     n_h - 隐藏层的数量\n",
    "     n_y - 输出层的数量\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] #输入层\n",
    "    n_h = 4 #，隐藏层，硬编码为4\n",
    "    n_y = Y.shape[0] #输出层\n",
    "\n",
    "    return (n_x,n_h,n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 初始化模型的参数\n",
    "\n",
    "用随机值初始化权重矩阵。\n",
    "\n",
    "- np.random.randn(a，b)* 0.01 \n",
    "\n",
    "将偏向量初始化为零。 \n",
    "- np.zeros((a，b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters( n_x , n_h ,n_y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        n_x - 输入层节点的数量\n",
    "        n_h - 隐藏层节点的数量\n",
    "        n_y - 输出层节点的数量\n",
    "\n",
    "    返回：\n",
    "        parameters - 包含参数的字典：\n",
    "            W1 - 权重矩阵,维度为（n_h，n_x）\n",
    "            b1 - 偏向量，维度为（n_h，1）\n",
    "            W2 - 权重矩阵，维度为（n_y，n_h）\n",
    "            b2 - 偏向量，维度为（n_y，1）\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(2) #指定一个随机种子，以便你的输出与我们的一样。\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "\n",
    "    #使用断言确保我的数据格式是正确的\n",
    "    assert(W1.shape == ( n_h , n_x ))\n",
    "    assert(b1.shape == ( n_h , 1 ))\n",
    "    assert(W2.shape == ( n_y , n_h ))\n",
    "    assert(b2.shape == ( n_y , 1 ))\n",
    "\n",
    "    parameters = {\"W1\" : W1,\n",
    "                  \"b1\" : b1,\n",
    "                  \"W2\" : W2,\n",
    "                  \"b2\" : b2 }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 前向传播\n",
    "我们现在要实现前向传播函数forward_propagation()。 \n",
    "我们可以使用sigmoid()函数，也可以使用np.tanh()函数。 \n",
    "步骤如下：\n",
    "\n",
    "- 使用字典类型的parameters（它是initialize_parameters() 的输出）检索每个参数。\n",
    "- 实现向前传播, 计算$Z^{[1]},A^{[1]},Z^{[2]},Z^{[2]}$ 和 $A^{[2]}$（ 训练集里面所有例子的预测向量）。\n",
    "- 反向传播所需的值存储在“cache”中，cache将作为反向传播函数的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation( X , parameters ):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "         X - 维度为（n_x，m）的输入数据。\n",
    "         parameters - 初始化函数（initialize_parameters）的输出\n",
    "\n",
    "    返回：\n",
    "         A2 - 使用sigmoid()函数计算的第二次激活后的数值\n",
    "         cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型变量\n",
    "     \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    #前向传播计算A2\n",
    "    Z1 = np.dot(W1 , X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2 , A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    #使用断言确保我的数据格式是正确的\n",
    "    assert(A2.shape == (1,X.shape[1]))\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "\n",
    "    return (A2, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 计算损失\n",
    "\n",
    "![](../image/0325.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2,Y,parameters):\n",
    "    \"\"\"\n",
    "    计算方程（6）中给出的交叉熵成本，\n",
    "\n",
    "    参数：\n",
    "         A2 - 使用sigmoid()函数计算的第二次激活后的数值\n",
    "         Y - \"True\"标签向量,维度为（1，数量）\n",
    "         parameters - 一个包含W1，B1，W2和B2的字典类型的变量\n",
    "\n",
    "    返回：\n",
    "         成本 - 交叉熵成本给出方程（13）\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    #计算成本\n",
    "    logprobs = logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    assert(isinstance(cost,float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08 向后传播\n",
    "\n",
    "![](../image/0326.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters,cache,X,Y):\n",
    "    \"\"\"\n",
    "    使用上述说明搭建反向传播函数。\n",
    "\n",
    "    参数：\n",
    "     parameters - 包含我们的参数的一个字典类型的变量。\n",
    "     cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型的变量。\n",
    "     X - 输入数据，维度为（2，数量）\n",
    "     Y - “True”标签，维度为（1，数量）\n",
    "\n",
    "    返回：\n",
    "     grads - 包含W和b的导数一个字典类型的变量。\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    dZ2= A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2 }\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 更新参数\n",
    "\n",
    "我们需要使用(dW1, db1, dW2, db2)来更新(W1, b1, W2, b2)。 \n",
    "更新算法如下：\n",
    "\n",
    "![](../image/0327.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def update_parameters(parameters,grads,learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    使用上面给出的梯度下降更新规则更新参数\n",
    "\n",
    "    参数：\n",
    "     parameters - 包含参数的字典类型的变量。\n",
    "     grads - 包含导数值的字典类型的变量。\n",
    "     learning_rate - 学习速率\n",
    "\n",
    "    返回：\n",
    "     parameters - 包含更新参数的字典类型的变量。\n",
    "    \"\"\"\n",
    "    W1,W2 = parameters[\"W1\"],parameters[\"W2\"]\n",
    "    b1,b2 = parameters[\"b1\"],parameters[\"b2\"]\n",
    "\n",
    "    dW1,dW2 = grads[\"dW1\"],grads[\"dW2\"]\n",
    "    db1,db2 = grads[\"db1\"],grads[\"db2\"]\n",
    "\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 整合\n",
    "\n",
    "我们现在把上面的东西整合到nn_model()中，神经网络模型必须以正确的顺序使用先前的功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X,Y,n_h,num_iterations,print_cost=False):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        X - 数据集,维度为（2，示例数）\n",
    "        Y - 标签，维度为（1，示例数）\n",
    "        n_h - 隐藏层的数量\n",
    "        num_iterations - 梯度下降循环中的迭代次数\n",
    "        print_cost - 如果为True，则每1000次迭代打印一次成本数值\n",
    "\n",
    "    返回：\n",
    "        parameters - 模型学习的参数，它们可以用来进行预测。\n",
    "     \"\"\"\n",
    "\n",
    "    np.random.seed(3) #指定随机种子\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "\n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        A2 , cache = forward_propagation(X,parameters)\n",
    "        cost = compute_cost(A2,Y,parameters)\n",
    "        grads = backward_propagation(parameters,cache,X,Y)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate = 0.5)\n",
    "\n",
    "        if print_cost:\n",
    "            if i%1000 == 0:\n",
    "                print(\"第 \",i,\" 次循环，成本为：\"+str(cost))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 预测\n",
    "\n",
    "构建predict()来使用模型进行预测， 使用向前传播来预测结果。\n",
    "\n",
    "![](../image/0328.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters,X):\n",
    "    \"\"\"\n",
    "    使用学习的参数，为X中的每个示例预测一个类\n",
    "\n",
    "    参数：\n",
    "        parameters - 包含参数的字典类型的变量。\n",
    "        X - 输入数据（n_x，m）\n",
    "\n",
    "    返回\n",
    "        predictions - 我们模型预测的向量（红色：0 /蓝色：1）\n",
    "\n",
    "     \"\"\"\n",
    "    A2 , cache = forward_propagation(X,parameters)\n",
    "    predictions = np.round(A2)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 正式运行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)\n",
    "\n",
    "#绘制边界\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))\n",
    "\n",
    "predictions = predict(parameters, X)\n",
    "print ('准确率: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50] #隐藏层数量\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations=5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100)\n",
    "    print (\"隐藏层的节点数量： {}  ，准确率: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "本文博客地址：https://blog.csdn.net/u013733326/article/details/79702148\n",
    "\n",
    "@author: Oscar\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
    "\n",
    "#%matplotlib inline #如果你使用用的是Jupyter Notebook的话请取消注释。\n",
    "\n",
    "np.random.seed(1) #设置一个固定的随机种子，以保证接下来的步骤中我们的结果是一致的。\n",
    "\n",
    "X, Y = load_planar_dataset()\n",
    "#plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral) #绘制散点图\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = Y.shape[1]  # 训练集里面的数量\n",
    "\n",
    "print (\"X的维度为: \" + str(shape_X))\n",
    "print (\"Y的维度为: \" + str(shape_Y))\n",
    "print (\"数据集里面的数据有：\" + str(m) + \" 个\")\n",
    "\n",
    "def layer_sizes(X , Y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "     X - 输入数据集,维度为（输入的数量，训练/测试的数量）\n",
    "     Y - 标签，维度为（输出的数量，训练/测试数量）\n",
    "\n",
    "    返回：\n",
    "     n_x - 输入层的数量\n",
    "     n_h - 隐藏层的数量\n",
    "     n_y - 输出层的数量\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] #输入层\n",
    "    n_h = 4 #，隐藏层，硬编码为4\n",
    "    n_y = Y.shape[0] #输出层\n",
    "\n",
    "    return (n_x,n_h,n_y)\n",
    "\n",
    "def initialize_parameters( n_x , n_h ,n_y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        n_x - 输入节点的数量\n",
    "        n_h - 隐藏层节点的数量\n",
    "        n_y - 输出层节点的数量\n",
    "\n",
    "    返回：\n",
    "        parameters - 包含参数的字典：\n",
    "            W1 - 权重矩阵,维度为（n_h，n_x）\n",
    "            b1 - 偏向量，维度为（n_h，1）\n",
    "            W2 - 权重矩阵，维度为（n_y，n_h）\n",
    "            b2 - 偏向量，维度为（n_y，1）\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(2) #指定一个随机种子，以便你的输出与我们的一样。\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "\n",
    "    #使用断言确保我的数据格式是正确的\n",
    "    assert(W1.shape == ( n_h , n_x ))\n",
    "    assert(b1.shape == ( n_h , 1 ))\n",
    "    assert(W2.shape == ( n_y , n_h ))\n",
    "    assert(b2.shape == ( n_y , 1 ))\n",
    "\n",
    "    parameters = {\"W1\" : W1,\n",
    "                  \"b1\" : b1,\n",
    "                  \"W2\" : W2,\n",
    "                  \"b2\" : b2 }\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation( X , parameters ):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "         X - 维度为（n_x，m）的输入数据。\n",
    "         parameters - 初始化函数（initialize_parameters）的输出\n",
    "\n",
    "    返回：\n",
    "         A2 - 使用sigmoid()函数计算的第二次激活后的数值\n",
    "         cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型变量\n",
    "     \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    #前向传播计算A2\n",
    "    Z1 = np.dot(W1 , X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2 , A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    #使用断言确保我的数据格式是正确的\n",
    "    assert(A2.shape == (1,X.shape[1]))\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "\n",
    "    return (A2, cache)\n",
    "\n",
    "def compute_cost(A2,Y,parameters):\n",
    "    \"\"\"\n",
    "    计算方程（6）中给出的交叉熵成本，\n",
    "\n",
    "    参数：\n",
    "         A2 - 使用sigmoid()函数计算的第二次激活后的数值\n",
    "         Y - \"True\"标签向量,维度为（1，数量）\n",
    "         parameters - 一个包含W1，B1，W2和B2的字典类型的变量\n",
    "\n",
    "    返回：\n",
    "         成本 - 交叉熵成本给出方程（13）\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    #计算成本\n",
    "    logprobs = logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    assert(isinstance(cost,float))\n",
    "\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters,cache,X,Y):\n",
    "    \"\"\"\n",
    "    使用上述说明搭建反向传播函数。\n",
    "\n",
    "    参数：\n",
    "     parameters - 包含我们的参数的一个字典类型的变量。\n",
    "     cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型的变量。\n",
    "     X - 输入数据，维度为（2，数量）\n",
    "     Y - “True”标签，维度为（1，数量）\n",
    "\n",
    "    返回：\n",
    "     grads - 包含W和b的导数一个字典类型的变量。\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "\n",
    "    dZ2= A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2 }\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters,grads,learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    使用上面给出的梯度下降更新规则更新参数\n",
    "\n",
    "    参数：\n",
    "     parameters - 包含参数的字典类型的变量。\n",
    "     grads - 包含导数值的字典类型的变量。\n",
    "     learning_rate - 学习速率\n",
    "\n",
    "    返回：\n",
    "     parameters - 包含更新参数的字典类型的变量。\n",
    "    \"\"\"\n",
    "    W1,W2 = parameters[\"W1\"],parameters[\"W2\"]\n",
    "    b1,b2 = parameters[\"b1\"],parameters[\"b2\"]\n",
    "\n",
    "    dW1,dW2 = grads[\"dW1\"],grads[\"dW2\"]\n",
    "    db1,db2 = grads[\"db1\"],grads[\"db2\"]\n",
    "\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def nn_model(X,Y,n_h,num_iterations,print_cost=False):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        X - 数据集,维度为（2，示例数）\n",
    "        Y - 标签，维度为（1，示例数）\n",
    "        n_h - 隐藏层的数量\n",
    "        num_iterations - 梯度下降循环中的迭代次数\n",
    "        print_cost - 如果为True，则每1000次迭代打印一次成本数值\n",
    "\n",
    "    返回：\n",
    "        parameters - 模型学习的参数，它们可以用来进行预测。\n",
    "     \"\"\"\n",
    "\n",
    "    np.random.seed(3) #指定随机种子\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "\n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        A2 , cache = forward_propagation(X,parameters)\n",
    "        cost = compute_cost(A2,Y,parameters)\n",
    "        grads = backward_propagation(parameters,cache,X,Y)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate = 0.5)\n",
    "\n",
    "        if print_cost:\n",
    "            if i%1000 == 0:\n",
    "                print(\"第 \",i,\" 次循环，成本为：\"+str(cost))\n",
    "    return parameters\n",
    "\n",
    "def predict(parameters,X):\n",
    "    \"\"\"\n",
    "    使用学习的参数，为X中的每个示例预测一个类\n",
    "\n",
    "    参数：\n",
    "        parameters - 包含参数的字典类型的变量。\n",
    "        X - 输入数据（n_x，m）\n",
    "\n",
    "    返回\n",
    "        predictions - 我们模型预测的向量（红色：0 /蓝色：1）\n",
    "\n",
    "     \"\"\"\n",
    "    A2 , cache = forward_propagation(X,parameters)\n",
    "    predictions = np.round(A2)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)\n",
    "\n",
    "#绘制边界\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))\n",
    "\n",
    "predictions = predict(parameters, X)\n",
    "print ('准确率: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50] #隐藏层数量\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations=5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100)\n",
    "    print (\"隐藏层的节点数量： {}  ，准确率: {} %\".format(n_h, accuracy))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
