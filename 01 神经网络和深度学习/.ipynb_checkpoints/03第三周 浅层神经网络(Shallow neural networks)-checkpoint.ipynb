{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三周：浅层神经网络(Shallow neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 本周主要内容\n",
    "- 3.1 神经网络概述（Neural Network Overview）\n",
    "- 3.2 神经网络的表示（Neural Network Representation）\n",
    "- 3.3 计算一个神经网络的输出（Computing a Neural Network's output）\n",
    "- 3.4 多样本向量化（Vectorizing across multiple examples）\n",
    "- 3.5 向 量 化 实 现 的 解 释 （Justification for vectorized implementation）\n",
    "- 3.6 激活函数（Activation functions）\n",
    "- 3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）\n",
    "- 3.8 激活函数的导数（Derivatives of activation functions）\n",
    "- 3.9 神 经 网 络 的 梯 度 下 降 （Gradient descent for neural networks）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 神经网络概述（Neural Network Overview）\n",
    "\n",
    "现在我们开始快速浏览一下如何实现神经网络。上周我们讨论了逻辑回归，我们了解了\n",
    "这个模型如何与下面公式建立联系。\n",
    "\n",
    "![](../image/0301.png)\n",
    "\n",
    "![](../image/0302.png)\n",
    "\n",
    "\n",
    "先你需要输入特征x，参数w和b，通过这些你就可以计算出z,通过z就可以\n",
    "计算出a，然后求出loss function L(a,y)\n",
    "\n",
    "下图就是神经网络的基本结构，将许多的sigmoid单元堆叠起来就形成了一个\n",
    "神经网络。\n",
    "![](../image/0303.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 神经网络的表示（Neural Network Representation）\n",
    "\n",
    "下图是一张神经网络的图片：\n",
    "![](../image/0304.png)\n",
    "\n",
    "输入特征$x_1,x_2,x_3$，叫做`输入层`,它包含了神经网络的所有输入；接下来得一层称为`隐藏层`,最后一层为`输出层`。\n",
    "\n",
    "![](../image/0305.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.3 计算一个神经网络的输出（Computing a Neural Network's output）\n",
    "\n",
    "#### 神经元的计算\n",
    "如下图所示，用圆圈表示神经完了的计算单元，逻辑回归的计算有两个\n",
    "步骤：先计算z，然后计算a，一个神经网络就是多次重复这个计算。\n",
    "\n",
    "![](../image/0306.png)\n",
    "\n",
    "![](../image/0307.png)\n",
    "\n",
    "向量化计算\n",
    "\n",
    "![](../image/0308.png)\n",
    "\n",
    "![](../image/0309.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 多样本向量化（Vectorizing across multiple examples）\n",
    "\n",
    "![](../image/0310.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 向 量 化 实 现 的 解 释 （Justification for vectorized implementation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 激活函数（Activation functions）\n",
    "\n",
    "使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。\n",
    "到目前为止，之前的视频只用过 sigmoid 激活函数，但是，有时其他的激活函数效果会更好。\n",
    "\n",
    "更通常的情况下，使用不同的激活函数，激活函数可以是除了 sigmoid 函数意外的非线性函\n",
    "数。 tanh 函数或者双曲正切函数是总体上都优于 sigmoid 函数的激活函数\n",
    "\n",
    "在讨论优化算法时，有一点要说明：我基本已经不用 sigmoid 激活函数了， tanh 函数在\n",
    "所有场合都优于 sigmoid 函数。\n",
    "\n",
    "### 不同激活函数的过程和结论：\n",
    "- sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。\n",
    "- tanh 激活函数： tanh 是非常优秀的，几乎适合所有场合。\n",
    "- ReLu 激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu。\n",
    "![](../image/0312.png)\n",
    "![](../image/0311.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）\n",
    "![](../image/0313.png)\n",
    "\n",
    "我们稍后会谈到深度网络，有很多层的神经网络，很多隐藏层。事实证明，如果你使用\n",
    "线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是\n",
    "计算线性函数，所以不如直接去掉全部隐藏层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 激活函数的导数（Derivatives of activation functions）\n",
    "\n",
    "![](../image/0314.png)\n",
    "![](../image/0315.png)\n",
    "![](../image/0316.png)\n",
    "![](../image/0317.png)\n",
    "![](../image/0318.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 神 经 网 络 的 梯 度 下 降 （Gradient descent for neural networks）\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
