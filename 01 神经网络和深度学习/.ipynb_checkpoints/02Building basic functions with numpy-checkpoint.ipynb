{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 sigmoid函数\n",
    "\n",
    "sigmoid函数的定义：$sigmoid(x)= \\sigma(x)=\\frac{1}{1+e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1 2 3]\n",
      "sigmoid(x)= [0.73105858 0.88079708 0.95257413]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "print('x=',x)\n",
    "print('sigmoid(x)=',sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 sigmoid函数的导数\n",
    "\n",
    "$sigmoid\\_derivative(x) = \\sigma^{'}(x) = \\sigma(x)(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "    ds = s * (1-s)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Reshaping arrays\n",
    "\n",
    " - `X.shape` 用来获取X的shape(dimension)\n",
    " - `X.reshape`用来改变X的维度\n",
    " \n",
    " 例如: 一幅图像的数据是一个3维数组(length; height; depth = 3)，\n",
    " 在实际运用中，需要转化为一个1维数组 (length ∗ height ∗ 3; 1)\n",
    " \n",
    " 练习：实现“image2vector()” 函数， (length, height,3)的数据\n",
    "转化为 (lengthheight3, 1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Normalizing rows\n",
    "\n",
    "正则化数据是机器学习和深度学习中另一个通用的技术，正则化后梯度下降的\n",
    "执行速度会更快。\n",
    "\n",
    "**正则化：将x转化为\n",
    "$\\frac{x}{||x||} $**\n",
    "\n",
    "例如：\n",
    "\n",
    "![](../image/0239.png)\n",
    "\n",
    "练习：实现normalizeRows()函数，对一个矩阵进行行正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nomalizeRows(x): [[0.         0.4472136  0.89442719]\n",
      " [0.55708601 0.74278135 0.37139068]]\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "    x_norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    x = x/x_norm\n",
    "    return x\n",
    "\n",
    "x = np.array([\n",
    "    [0,2,4],\n",
    "    [3,4,2]\n",
    "])\n",
    "\n",
    "print(\"nomalizeRows(x):\",normalizeRows(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Broadcasting and the softmax function \n",
    "\n",
    "**“Broadcasting”**是numpy中一个非常重要的概念，它对两个不同形状的数组进行数学操作是非常有用的。\n",
    "\n",
    "**softmax function:**\n",
    "\n",
    "当$x\\in \\mathbb{R}^{^1\\times n}$ 时,\n",
    "\n",
    "$softmax(x)=sotfmax([x_1,x_2,...x_n])= \\left [ \\frac{e^{x_1}}{\\sum_j e^{x_j}},\\frac{e^{x_2}}{\\sum_j e^{x_j}} ...\\frac{e^{x_n}}{\\sum_j e^{x_j}} \\right ]$\n",
    "\n",
    "当$x\\in \\mathbb{R}^{^m\\times n}$ 时,\n",
    "\n",
    "![](../image/0240.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "# Calculates the softmax for each row of the input x.\n",
    "# Your code should work for a row vector and also for matrices of\n",
    "# ,! shape (n, m).\n",
    "# Argument:\n",
    "# x -- A numpy matrix of shape (n,m)\n",
    "# Returns:\n",
    "# s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use,! np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should,! automatically use numpy broadcasting.\n",
    "    s = x_exp/x_sum\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Implement the L1 and L2 loss functions\n",
    "\n",
    "**L1:  $L_1(\\hat{y},y)=\\sum_{i=0}^{m}\\left|y^{(i)}-{\\hat{y}^{(1)}}\\right|$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "def L1(yhat,y):\n",
    "    loss = sum(abs(y-yhat))\n",
    "    return loss\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1:  $L_2(\\hat{y},y)=\\sum_{i=0}^{m}{(y^{(i)}-{\\hat{y}^{(1)}})}^2$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "def L2(yhat,y):\n",
    "    loss = np.dot(y-yhat,y-yhat)\n",
    "    return loss\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
