{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三周：浅层神经网络(Shallow neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 本周主要内容\n",
    "- 3.1 神经网络概述（Neural Network Overview）\n",
    "- 3.2 神经网络的表示（Neural Network Representation）\n",
    "- 3.3 计算一个神经网络的输出（Computing a Neural Network's output）\n",
    "- 3.4 多样本向量化（Vectorizing across multiple examples）\n",
    "- 3.5 向 量 化 实 现 的 解 释 （Justification for vectorized implementation）\n",
    "- 3.6 激活函数（Activation functions）\n",
    "- 3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）\n",
    "- 3.8 激活函数的导数（Derivatives of activation functions）\n",
    "- 3.9 神 经 网 络 的 梯 度 下 降 （Gradient descent for neural networks）\n",
    "- 3.10（选修）直观理解反向传播（Backpropagation intuition）\n",
    "- 3.11 随机初始化（Random+Initialization）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 神经网络概述（Neural Network Overview）\n",
    "\n",
    "现在我们开始快速浏览一下如何实现神经网络。上周我们讨论了逻辑回归，我们了解了\n",
    "这个模型如何与下面公式建立联系。\n",
    "\n",
    "![](../image/0301.png)\n",
    "\n",
    "![](../image/0302.png)\n",
    "\n",
    "\n",
    "先你需要输入特征x，参数w和b，通过这些你就可以计算出z,通过z就可以\n",
    "计算出a，然后求出loss function L(a,y)\n",
    "\n",
    "下图就是神经网络的基本结构，将许多的sigmoid单元堆叠起来就形成了一个\n",
    "神经网络。\n",
    "![](../image/0303.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 神经网络的表示（Neural Network Representation）\n",
    "\n",
    "下图是一张神经网络的图片：\n",
    "![](../image/0304.png)\n",
    "\n",
    "输入特征$x_1,x_2,x_3$，叫做`输入层`,它包含了神经网络的所有输入；接下来得一层称为`隐藏层`,最后一层为`输出层`。\n",
    "\n",
    "![](../image/0305.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.3 计算一个神经网络的输出（Computing a Neural Network's output）\n",
    "\n",
    "#### 神经元的计算\n",
    "如下图所示，用圆圈表示神经完了的计算单元，逻辑回归的计算有两个\n",
    "步骤：先计算z，然后计算a，一个神经网络就是多次重复这个计算。\n",
    "\n",
    "![](../image/0306.png)\n",
    "\n",
    "![](../image/0307.png)\n",
    "\n",
    "向量化计算\n",
    "\n",
    "![](../image/0308.png)\n",
    "\n",
    "![](../image/0309.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 多样本向量化（Vectorizing across multiple examples）\n",
    "\n",
    "![](../image/0310.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 向 量 化 实 现 的 解 释 （Justification for vectorized implementation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 激活函数（Activation functions）\n",
    "\n",
    "使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。\n",
    "到目前为止，之前的视频只用过 sigmoid 激活函数，但是，有时其他的激活函数效果会更好。\n",
    "\n",
    "更通常的情况下，使用不同的激活函数，激活函数可以是除了 sigmoid 函数意外的非线性函\n",
    "数。 tanh 函数或者双曲正切函数是总体上都优于 sigmoid 函数的激活函数\n",
    "\n",
    "在讨论优化算法时，有一点要说明：我基本已经不用 sigmoid 激活函数了， tanh 函数在\n",
    "所有场合都优于 sigmoid 函数。\n",
    "\n",
    "### 不同激活函数的过程和结论：\n",
    "- sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。\n",
    "- tanh 激活函数： tanh 是非常优秀的，几乎适合所有场合。\n",
    "- ReLu 激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu。\n",
    "![](../image/0312.png)\n",
    "![](../image/0311.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）\n",
    "![](../image/0313.png)\n",
    "\n",
    "我们稍后会谈到深度网络，有很多层的神经网络，很多隐藏层。事实证明，如果你使用\n",
    "线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是\n",
    "计算线性函数，所以不如直接去掉全部隐藏层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 激活函数的导数（Derivatives of activation functions）\n",
    "\n",
    "![](../image/0314.png)\n",
    "![](../image/0315.png)\n",
    "![](../image/0316.png)\n",
    "![](../image/0317.png)\n",
    "![](../image/0318.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 神 经 网 络 的 梯 度 下 降 （Gradient descent for neural networks）\n",
    "\n",
    "假设做一个二分类任务，成本函数和之前做逻辑回归的loss function一样，\n",
    "等于：\n",
    "\n",
    "Cost function: $J(W^{[1]},b^{[2]},W^{[2]},b^{[2]})=\\frac{1}{m}\\sum_{i=1}^mL(\\hat y,y)$\n",
    "\n",
    "训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而不是初\n",
    "始化成全零。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：${\\hat y}^{(i)},(i=1,2,...,m)$\n",
    "\n",
    "### 求梯度：\n",
    "\n",
    "$dW^{[1]}=\\frac{dJ}{dW^{[1]}}$ ,$db^{[1]}=\\frac{dJ}{db^{[1]}}$\n",
    "\n",
    "$dW^{[2]}=\\frac{dJ}{dW^{[2]}}$ ,$db^{[2]}=\\frac{dJ}{db^{[2]}}$\n",
    "\n",
    "### 更新：\n",
    "\n",
    "$W^{[1]}:=W^{[1]}-\\alpha dW^{[1]}$ ,$b^{[1]}:=b^{[1]}-\\alpha db^{[1]}$\n",
    "\n",
    "$W^{[2]}:=W^{[2]}-\\alpha dW^{[2]}$ ,$b^{[2]}:=b^{[2]}-\\alpha db^{[2]}$\n",
    "\n",
    "\n",
    "### 正向传播方程（forward propagation）:\n",
    "\n",
    " $z^{[1]}=W^{[1]}x+b^{[1]}$\n",
    " \n",
    " $a^{[1]}=\\sigma (z^{[1]})$\n",
    "\n",
    " $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$\n",
    " \n",
    " $a^{[2]}=\\sigma (z^{[2]})$\n",
    "\n",
    "### 反向传播方程\n",
    "\n",
    "$dz^{[2]}=A^{[2]}-Y, Y=[y^{[1]},y^{[2]},...,y^{[m]}]$\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m}dz^{[2]}A^{[1]T}$\n",
    "\n",
    "$db^{[2]} = \\frac{1}{m} np.sum(dz^{[2]},axis=1,keepdims=True)$\n",
    "\n",
    "$dz^{[1]} = W^{[2]T}dz^{[2]}*{g^{[1]}}' *z^{[1]}$\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m}dz^{[1]}x^{T}$\n",
    "\n",
    "$db^{[1]} = \\frac{1}{m} np.sum(dz^{[1]},axis=1,keepdims=True)$\n",
    "\n",
    "\n",
    "注：这些都是针对所有样本进行过向量化， Y是$1\\times m$的矩阵；\n",
    "这里 np.sum 是 python 的 numpy 命令， axis=1 表示水平相加求和， keepdims 是防止\n",
    "python 输出那些古怪的秩数(n, )，加上这个确保阵矩阵$db^{[2]}$这个向量输出的维度为(n, 1)这\n",
    "样标准的形式。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10（选修）直观理解反向传播（Backpropagation intuition）\n",
    "\n",
    "![](../image/0319.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 随机初始化（Random+Initialization）\n",
    "\n",
    "当你训练神经网络时，权重随机初始化是很重要的。对于逻辑回归，把权重初始化为 0\n",
    "当然也是可以的。但是对于一个神经网络，如果你把权重或者参数都初始化为 0，那么梯度\n",
    "下降将不会起作用。\n",
    "\n",
    " 你 应 该 这 么 做 ： 把 $W^{[1]}$设 为\n",
    "np.random.randn(2,2)(生成高斯分布)，通常再乘上一个小的数，比如 0.01，这样把它初\n",
    "始化为很小的随机数。然后b没有这个对称的问题（叫做 symmetry breaking problem），所\n",
    "以可以把 b初始化为 0，因为只要随机初始化W你就有不同的隐含单元计算不同的东西，\n",
    "因此不会有 symmetry breaking 问题了。相似的，对于$W^{[2]}$你可以随机初始化， $b^{[2]}$可以初始\n",
    "化为 0。\n",
    "\n",
    "![](../image/0320.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
