{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 训练，验证，测试集（Train / Dev / Test sets）\n",
    "\n",
    "我们将所有的样本数据分成三个部分：Train/Dev/Test sets。Train sets用来训练你的算法模型；Dev sets用来验证不同算法的表现情况，从中选择最好的算法模型；Test sets用来测试最好算法的实际表现，作为该算法的无偏估计。\n",
    "\n",
    "之前人们通常设置Train sets和Test sets的数量比例为70%和30%。如果有Dev sets，则设置比例为60%、20%、20%，分别对应Train/Dev/Test sets。这种比例分配在样本数量不是很大的情况下，例如100,1000,10000，是比较科学的。但是如果数据量很大的时候，例如100万，这种比例分配就不太合适了。科学的做法是要将Dev sets和Test sets的比例设置得很低。因为Dev sets的目标是用来比较验证不同算法的优劣，从而选择更好的算法模型就行了。因此，通常不需要所有样本的20%这么多的数据来进行验证。对于100万的样本，往往只需要10000个样本来做验证就够了。Test sets也是一样，目标是测试已选算法的实际表现，无偏估计。对于100万的样本，往往也只需要10000个样本就够了。因此，对于大数据样本，Train/Dev/Test sets的比例通常可以设置为98%/1%/1%，或者99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例可以设置的越低一些。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 偏差，方差（Bias /Variance）\n",
    "偏差（Bias）和方差（Variance）是机器学习领域非常重要的两个概念和需要解决的问题。在传统的机器学习算法中，Bias和Variance是对立的，分别对应着欠拟合和过拟合，我们常常需要在Bias和Variance之间进行权衡。而在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。\n",
    "\n",
    "如下图所示，显示了二维平面上，high bias，just right，high variance的例子。可见，high bias对应着欠拟合，而high variance对应着过拟合。\n",
    "![](../image/0403.png)\n",
    "\n",
    "上图这个例子中输入特征是二维的，high bias和high variance可以直接从图中分类线看出来。而对于输入特征是高维的情况，如何来判断是否出现了high bias或者high variance呢？\n",
    "\n",
    "例如猫识别问题，输入是一幅图像，其特征维度很大。这种情况下，我们可以通过两个数值Train set error和Dev set error来理解bias和variance。假设Train set error为1%，而Dev set error为11%，即该算法模型对训练样本的识别很好，但是对验证集的识别却不太好。这说明了该模型对训练样本可能存在过拟合，模型泛化能力不强，导致验证集识别率低。这恰恰是high variance的表现。假设Train set error为15%，而Dev set error为16%，虽然二者error接近，即该算法模型对训练样本和验证集的识别都不是太好。这说明了该模型对训练样本存在欠拟合。这恰恰是high bias的表现。假设Train set error为15%，而Dev set error为30%，说明了该模型既存在high bias也存在high variance（深度学习中最坏的情况）。再假设Train set error为0.5%，而Dev set error为1%，即low bias和low variance，是最好的情况。值得一提的是，以上的这些假设都是建立在base error是0的基础上，即人类都能正确识别所有猫类图片。base error不同，相应的Train set error和Dev set error会有所变化，但没有相对变化。\n",
    "\n",
    "一般来说，Train set error体现了是否出现bias，Dev set error体现了是否出现variance（正确地说，应该是Dev set error与Train set error的相对差值）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 机器学习基础（Basic Recipe for Machine Learning）\n",
    "\n",
    "机器学习中基本的一个诀窍就是避免出现high bias和high variance。首先，减少high bias的方法通常是增加神经网络的隐藏层个数、神经元个数，训练时间延长，选择其它更复杂的NN模型等。在base error不高的情况下，一般都能通过这些方式有效降低和避免high bias，至少在训练集上表现良好。其次，减少high variance的方法通常是增加训练样本数据，进行正则化Regularization，选择其他更复杂的NN模型等。\n",
    "\n",
    "这里有几点需要注意的。第一，解决high bias和high variance的方法是不同的。实际应用中通过Train set error和Dev set error判断是否出现了high bias或者high variance，然后再选择针对性的方法解决问题。\n",
    "\n",
    "第二，Bias和Variance的折中tradeoff。传统机器学习算法中，Bias和Variance通常是对立的，减小Bias会增加Variance，减小Variance会增加Bias。而在现在的深度学习中，通过使用更复杂的神经网络和海量的训练样本，一般能够同时有效减小Bias和Variance。这也是深度学习之所以如此强大的原因之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 正则化（Regularization）\n",
    "\n",
    "深度学习可能存在过拟合问题——高方差， 有两个解决方法， 一个是正则化， 另一个是\n",
    "准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者\n",
    "获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。\n",
    "下面介绍正则化：\n",
    "\n",
    "L2 regularization，其表达式为：\n",
    "![](../image/0404.png)\n",
    "\n",
    "这里有个问题：为什么只对w进行正则化而不对b进行正则化呢？其实也可以对b进行正则化。但是一般w的维度很大，而b只是一个常数。相比较来说，参数很大程度上由w决定，改变b值对整体模型影响较小。所以，一般为了简便，就忽略对b的正则化了。\n",
    "\n",
    "除了L2 regularization之外，还有另外一只正则化方法：L1 regularization。其表达式为：\n",
    "![](../image/0405.png)\n",
    "\n",
    "与L2 regularization相比，L1 regularization得到的w更加稀疏，即很多w为零值。其优点是节约存储空间，因为大部分w为0。然而，实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用。\n",
    "\n",
    "L1、L2 regularization中的λ就是正则化参数（超参数的一种）。可以设置λλ为不同的值，在Dev set中进行验证，选择最佳的λ。\n",
    "\n",
    "在深度学习模型中，L2 regularization的表达式为：\n",
    "![](../image/0406.png)\n",
    "![](../image/0407.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 为 什 么 正 则 化 有 利 于 预 防 过 拟 合 呢 ？ （Why regularization reduces overfitting?）\n",
    "\n",
    "为什么正则化能够有效避免high variance，防止过拟合呢？下面我们通过几个例子说明。\n",
    "还是之前那张图，从左到右，分别表示了欠拟合，刚好拟合，过拟合三种情况。\n",
    "\n",
    "![](../image/0408.png)\n",
    "\n",
    "假如我们选择了非常复杂的神经网络模型，如上图左上角所示。在未使用正则化的情况下，我们得到的分类超平面可能是类似上图右侧的过拟合。但是，如果使用L2 regularization，当λ很大时，$w^{[l]}≈0。w^{[l]}$近似为零，意味着该神经网络模型中的某些神经元实际的作用很小，可以忽略。从效果上来看，其实是将某些神经元给忽略掉了。这样原本过于复杂的神经网络模型就变得不那么复杂了，而变得非常简单化了。如下图所示，整个简化的神经网络模型变成了一个逻辑回归模型。问题就从high variance变成了high bias了。\n",
    "\n",
    "![](../image/0409.png)\n",
    "\n",
    "因此，选择合适大小的λλ值，就能够同时避免high bias和high variance，得到最佳模型。\n",
    "\n",
    "还有另外一个直观的例子来解释为什么正则化能够避免发生过拟合。假设激活函数是tanh函数。tanh函数的特点是在z接近零的区域，函数近似是线性的，而当|z|很大的时候，函数非线性且变化缓慢。当使用正则化，λ较大，即对权重$w^{[l]}$的惩罚较大，$w^{[l]}$减小。因为$z^{[l]}=w^{[l]}a^{[l]}+b^{[l]}$。当$w^{[l]}$减小的时候，$z^{[l]}$也会减小。则此时的$z^{[l]}$分布在tanh函数的近似线性区域。那么这个神经元起的作用就相当于是linear regression。如果每个神经元对应的权重$w^{[l]}$都比较小，那么整个神经网络模型相当于是多个linear regression的组合，即可看成一个linear network。得到的分类超平面就会比较简单，不会出现过拟合现象。\n",
    "\n",
    "![](../image/0410.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 dropout 正则化（Dropout Regularization）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了L2 regularization之外，还有另外一种防止过拟合的有效方法：Dropout。\n",
    "\n",
    "Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0411.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout有不同的实现方法，接下来介绍一种常用的方法：Inverted dropout。假设对于第ll层神经元，设定保留神经元比例概率keep_prob=0.8，即该层有20%的神经元停止工作。dldl为dropout向量，设置dldl为随机vector，其中80%的元素为1，20%的元素为0。在python中可以使用如下语句生成dropout vector："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dl = np.random.rand(al.shape[0],al.shape[1])<keep_prob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，第l层经过dropout，随机删减20%的神经元，只保留80%的神经元，其输出为：\n",
    "```\n",
    "al = np.multiply(al,dl)\n",
    "```\n",
    "最后，还要对al进行scale up处理，即：\n",
    "```\n",
    "al /= keep_prob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是Inverted dropout的方法。之所以要对al进行scale up是为了保证在经过dropout后，al作为下一层神经元的输入值尽量保持不变。假设第ll层有50个神经元，经过dropout后，有10个神经元停止工作，这样只有40神经元有作用。那么得到的alal只相当于原来的80%。scale up后，能够尽可能保持alal的期望值相比之前没有大的变化。\n",
    "\n",
    "Inverted dropout的另外一个好处就是在对该dropout后的神经网络进行测试时能够减少scaling问题。因为在训练时，使用scale up保证alal的期望值没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作了。\n",
    "\n",
    "对于m个样本，单次迭代训练时，随机删除掉隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新权重w和常数项b；接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新w和b。不断重复上述过程，直至迭代训练完成。\n",
    "\n",
    "值得注意的是，使用dropout训练结束后，在测试和实际应用模型时，不需要进行dropout和随机删减神经元，所有的神经元都在工作。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 理解 dropout（Understanding Dropout）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout 可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用\n",
    "呢？\n",
    "\n",
    "直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单\n",
    "元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重， dropout\n",
    "将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施 dropout 的结果使它会\n",
    "压缩权重，并完成一些预防过拟合的外层正则化； L2对不同权重的衰减是不同的，它取决于\n",
    "激活函数倍增的大小。\n",
    "总结一下， dropout 的功能类似于L2正则化，与L2正则化不同的是应用方式不同会带来\n",
    "一点点小变化，甚至更适用于不同的输入范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个直观认识是，我们从单个神经元入手，如图，这个单元的工作就是输入并生成一\n",
    "些有意义的输出。通过 dropout，该单元的输入几乎被消除，有时这两个单元会被删除，有\n",
    "时会删除其它单元，就是说，我用紫色圈起来的这个单元，它不能依靠任何特征，因为特征\n",
    "都有可能被随机清除，或者说该单元的输入也都可能被随机清除。我不愿意把所有赌注都放\n",
    "在一个节点上，不愿意给任何一个输入加上太多权重，因为它可能会被删除，因此该单元将\n",
    "通过这种方式积极地传播开，并为单元的四个输入增加一点权重，通过传播所有权重，\n",
    "dropout 将产生收缩权重的平方范数的效果，和我们之前讲过的L2正则化类似，实施 dropout\n",
    "的结果是它会压缩权重，并完成一些预防过拟合的外层正则化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0412.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实证明， dropout 被正式地作为一种正则化的替代形式， L2对不同权重的衰减是不同\n",
    "的，它取决于倍增的激活函数的大小。\n",
    "\n",
    "总结一下， dropout 的功能类似于L2正则化，与L2正则化不同的是，被应用的方式不同，\n",
    "dropout 也会有所不同，甚至更适用于不同的输入范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout 一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，\n",
    "如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代\n",
    "后都会下降，因为我们所优化的代价函数 J 实际上并没有明确定义，或者说在某种程度上很\n",
    "难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭 dropout 函数，将 keepprob 的值设为 1，运行代码，确保J函数单调递减。然后打开 dropout 函数，希望在 dropout\n",
    "过程中，代码并未引入 bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法\n",
    "性能的数据统计， 但你可以把它们与 dropout 方法一起使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 其他正则化方法（Other regularization methods）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了L2 regularization和dropout regularization之外，还有其它减少过拟合的方法。\n",
    "\n",
    "一种方法是增加训练样本数量。但是通常成本较高，难以获得额外的训练样本。但是，我们可以对已有的训练样本进行一些处理来“制造”出更多的样本，称为data augmentation。例如图片识别问题中，可以对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。如下图所示，这些处理都能“制造”出新的训练样本。虽然这些是基于原有样本的，但是对增大训练样本数量还是有很有帮助的，不需要增加额外成本，却能起到防止过拟合的效果。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0413.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数字识别中，也可以将原有的数字图片进行任意旋转或者扭曲，或者增加一些noise，如下图所示：\n",
    "\n",
    "![](../image/0414.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有另外一种防止过拟合的方法：early stopping。一个神经网络模型随着迭代训练次数增加，train set error一般是单调减小的，而dev set error 先减小，之后又增大。也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。因此，迭代训练次数不是越多越好，可以通过train set error和dev set error随着迭代次数的变化趋势，选择合适的迭代次数，即early stopping。\n",
    "\n",
    "然而，Early stopping有其自身缺点。通常来说，机器学习训练模型有两个目标：一是优化cost function，尽量减小J；二是防止过拟合。这两个目标彼此对立的，即减小J的同时可能会造成过拟合，反之亦然。我们把这二者之间的关系称为正交化orthogonalization。该节课开始部分就讲过，在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。但是，Early stopping的做法通过减少得带训练次数来防止过拟合，这样J就不会足够小。也就是说，early stopping将上述两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。\n",
    "\n",
    "与early stopping相比，L2 regularization可以实现“分而治之”的效果：迭代训练足够多，减小J，而且也能有效防止过拟合。而L2 regularization的缺点之一是最优的正则化参数λλ的选择比较复杂。对这一点来说，early stopping比较简单。总的来说，L2 regularization更加常用一些。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 归一化输入（Normalizing inputs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练神经网络时，标准化输入可以提高训练的速度。标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值μ后，再除以其方差$σ^2$："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0415.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的$μ$和$σ^2$对其进行标准化处理。这样保证了训练集合测试集的标准化操作一致。\n",
    "\n",
    "之所以要对输入进行标准化操作，主要是为了让所有输入归一化同样的尺度上，方便进行梯度下降算法时能够更快更准确地找到全局最优解。假如输入特征是二维的，且x1的范围是[1,1000]，x2的范围是[0,1]。如果不进行标准化处理，x1与x2之间分布极不平衡，训练得到的w1和w2也会在数量级上差别很大。这样导致的结果是cost function与w和b的关系可能是一个非常细长的椭圆形碗。对其进行梯度下降算法时，由于w1和w2数值差异很大，只能选择很小的学习因子α，来避免J发生振荡。一旦αα较大，必然发生振荡，J不再单调下降。如下左图所示。\n",
    "\n",
    "然而，如果进行了标准化操作，x1与x2分布均匀，w1和w2数值差别不大，得到的cost function与w和b的关系是类似圆形碗。对其进行梯度下降算法时，α可以选择相对大一些，且J一般不会发生振荡，保证了J是单调下降的。如下右图所示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0416.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一种情况，如果输入特征之间的范围本来就比较接近，那么不进行标准化操作也是没有太大影响的。但是，标准化处理在大多数场合下还是值得推荐的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络尤其是深度神经网络中存在可能存在这样一个问题：梯度消失和梯度爆炸。意思是当训练一个 层数非常多的神经网络时，计算得到的梯度可能非常小或非常大，甚至是指数级别的减小或增大。这样会让训练过程变得非常困难。\n",
    "\n",
    "举个例子来说明，假设一个多层的每层只包含两个神经元的深度神经网络模型，如下图所示：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0417.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化复杂度，便于分析，我们令各层的激活函数为线性函数，即$g(Z)=Z$。且忽略各层常数项b的影响，令$b$全部为零。那么，该网络的预测输出$\\hat Y$为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0418.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果各层权重$W^{[l]}$的元素都稍大于1，例如1.5，则预测输出$\\hat Y$将正比于$1.5^L$。L越大，$\\hat Y$越大，且呈指数型增长。我们称之为数值爆炸。相反，如果各层权重$W^{[l]}$的元素都稍小于1，例如0.5，则预测输出$\\hat Y$将正比于$0.5^L$。网络层数L越多，$\\hat Y$呈指数型减小。我们称之为数值消失。\n",
    "\n",
    "也就是说，如果各层权重$W^{[l]}$都大于1或者都小于1，那么各层激活函数的输出将随着层数l的增加，呈指数型增大或减小。当层数很大时，出现数值爆炸或消失。同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化。L非常大时，例如L=150，则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，这让训练过程十分困难。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing / Exploding gradients）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面介绍如何改善Vanishing and Exploding gradients这类问题，方法是对权重w进行一些初始化处理。\n",
    "\n",
    "深度神经网络模型中，以单个神经元为例，该层（l）的输入个数为n，其输出为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/0419.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里忽略了常数项b。为了让z不会过大或者过小，思路是让w与n有关，且n越大，w应该越小才好。这样能够保证z不会过大。一种方法是在初始化w时，令其方差为1/n。相应的python伪代码为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{[l]} = np.random.randn(shape)*np.sqrt(\\frac{1}{n^{[l-1]}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果激活函数是tanh，一般选择上面的初始化方法。\n",
    "\n",
    "如果激活函数是ReLU，权重w的初始化一般令其方差为2/n："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{[l]} = np.random.randn(shape)*np.sqrt(\\frac{2}{n^{[l-1]}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，Yoshua Bengio提出了另外一种初始化w的方法，令其方差为$\\frac{2}{n^{[l-1]}*n^{[l]}}$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{[l]} = np.random.randn(shape)*np.sqrt(\\frac{2}{n^{[l-1]}*n^{[l]}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至于选择哪种初始化方法因人而异，可以根据不同的激活函数选择不同方法。另外，我们可以对这些初始化方法中设置某些参数，作为超参数，通过验证集进行验证，得到最优参数，来优化神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 梯度的数值逼近（Numerical approximation of gradients）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
